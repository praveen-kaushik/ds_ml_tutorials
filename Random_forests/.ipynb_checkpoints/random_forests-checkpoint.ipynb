{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "\n",
    "Random forest is a  machine learning method with many applications ranging from healthcare to insurance. It can be used to model the impact of marketing on customer acquisition, retention, and churn or to predict disease risk and susceptibility in patients.\n",
    "\n",
    "Random forests (Breiman, 2001) is a substantial modification of [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) that builds a large collection of de-correlated trees, and then averages them. On many problems the performance of random forests is very similar to [boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)), and they are simpler to train and tune. As a consequence, random forests are popular.\n",
    "\n",
    "Random forest is capable of regression and classification. It can handle a large number of features, and it's helpful for estimating which of your variables are important in the underlying data being modeled.\n",
    "\n",
    "#### What is a Random Forest?\n",
    "\n",
    "A random forest is essentially a collection of decision trees, where each tree is slightly different from\n",
    "the others. The idea behind random forests is that each tree might do a relatively\n",
    "good job of predicting, but will likely overfit on part of the data. If we build many\n",
    "trees, all of which work well and overfit in different ways, we can reduce the amount\n",
    "of overfitting by averaging their results. This reduction in overfitting, while retaining\n",
    "the predictive power of the trees, can be shown using rigorous mathematics.\n",
    "To implement this strategy, we need to build many decision trees. Each tree should do\n",
    "an acceptable job of predicting the target, and should also be different from the other\n",
    "trees. Random forests get their name from injecting randomness into the tree build‐\n",
    "ing to ensure each tree is different. There are two ways in which the trees in a random\n",
    "forest are randomized: by selecting the data points used to build a tree and by select‐\n",
    "ing the features in each split test. Let’s go into this process in more detail.  \n",
    "\n",
    "#### Building random forests:  \n",
    "\n",
    "To build a random forest model, you need to decide on the\n",
    "number of trees to build (the `n_estimators` parameter of `RandomForestRegressor` or\n",
    "`RandomForestClassifier`). Let’s say we want to build 10 trees. These trees will be\n",
    "built completely independently from each other, and the algorithm will make differ‐\n",
    "ent random choices for each tree to make sure the trees are distinct. To build a tree,\n",
    "we first take what is called a bootstrap sample of our data. That is, from our `n_samples`\n",
    "data points, we repeatedly draw an example randomly with replacement (meaning the\n",
    "same sample can be picked multiple times), `n_samples` times. This will create a data‐\n",
    "set that is as big as the original dataset, but some data points will be missing from it\n",
    "(approximately one third), and some will be repeated.  \n",
    "\n",
    "To illustrate, let’s say we want to create a bootstrap sample of the list [1, 2, 3, 4] . A possible bootstrap sample would be [2, 4, 4, 3]. Another possible sample would be [4, 1, 4, 1].\n",
    "Next, a decision tree is built based on this newly created dataset. Next the decision tree algorithm looks for the best test for each node, in each node the algorithm randomly selects a subset of the features, and it looks for the best possible test involving one of these features. The number of features that are selected is controlled by the `max_features` parameter.\n",
    "This selection of a subset of features is repeated separately in each node, so that each\n",
    "node in a tree can make a decision using a different subset of the features.\n",
    "The bootstrap sampling leads to each decision tree in the random forest being built\n",
    "on a slightly different dataset. Because of the selection of features in each node, each\n",
    "split in each tree operates on a different subset of features. Together, these two mech‐\n",
    "anisms ensure that all the trees in the random forest are different.\n",
    "A critical parameter in this process is `max_features` . If we set `max_features` to `n_fea\n",
    "tures` , that means that each split can look at all features in the dataset, and no ran‐\n",
    "domness will be injected in the feature selection (the randomness due to the\n",
    "bootstrapping remains, though). If we set `max_features` to 1 , that means that the\n",
    "splits have no choice at all on which feature to test, and can only search over different\n",
    "thresholds for the feature that was selected randomly. Therefore, a high `max_fea\n",
    "tures` means that the trees in the random forest will be quite similar, and they will be\n",
    "able to fit the data easily, using the most distinctive features. A low `max_features`\n",
    "means that the trees in the random forest will be quite different, and that each tree\n",
    "might need to be very deep in order to fit the data well.\n",
    "To make a prediction using the random forest, the algorithm first makes a prediction\n",
    "for every tree in the forest. For regression, we can average these results to get our final\n",
    "prediction. For classification, a “soft voting” strategy is used. This means each algo‐\n",
    "rithm makes a “soft” prediction, providing a probability for each possible output label. The probabilities predicted by all the trees are averaged, and the class with the\n",
    "highest probability is predicted.\n",
    "\n",
    "\n",
    "#### Features of Random forests:\n",
    "\n",
    "some of the Features of Random Forests which make it so useful are: \n",
    "\n",
    "- It is unexcelled in accuracy among current algorithms.\n",
    "- It runs efficiently on large datasets.\n",
    "- It can handle thousands of input variables without variable deletion.\n",
    "- It gives estimates of what variables are important in the classification.\n",
    "- It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "- It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "- It has methods for balancing error in class population unbalanced data sets.\n",
    "- Generated forests can be saved for future use on other data.\n",
    "- Prototypes are computed that give information about the relation between the variables and the classification.\n",
    "- It computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.\n",
    "- The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "- It offers an experimental method for detecting variable interactions.\n",
    "- Random forests does not overfit. You can run as many trees as you want.  \n",
    "\n",
    "#### Random Forest algorithm:\n",
    "\n",
    "Random forest is like bootstrapping algorithm with Decision tree model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART model with different sample and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.  \n",
    "\n",
    "![](http://slideplayer.com/slide/7835984/25/images/26/Random+Forest:+Algorithm+Steps.jpg)  \n",
    "  \n",
    "#### Analysing Random forests: with an example:   \n",
    "\n",
    "For this example we’ll use the python package scikit-learn and its implementation of the random forest classifier. As you shall see, it is extremely easy to use.\n",
    "\n",
    "Let’s start by creating our dataset. Let us create a dataset of furnitures consisting of seats and beanchs and the task will be to classify them appropriatly.   \n",
    "We’ll work with the following characteristics:\n",
    "\n",
    "1. Color\n",
    "2. Length of Legs\n",
    "3. Area of top surface\n",
    "\n",
    "We can add as many other features if we would like, such as number of legs, age, location, and so on, but we’ll keep it simple. \n",
    "\n",
    "We will represent all possibilities of each attribute as a number. The colors of our beanchs and seats will be drawn from the same uniform distribution of, say, integers 0 through 5, where you can think of 0 as being brown, 1 as black, etc. We’ll pull the length of the legs from different uniform distributions of real numbers: between 2 and 5 for seats, and between 4 and 10 for beanchs. Finally, the area of the top surface will draw from a normal distribution: for seats, we’ll use mean 2 and standard deviation 0.25; for beanchs, mean 5 and standard deviation 1.\n",
    "\n",
    "Let’s construct our data in python. The classifier expects two lists: one containing the data and the other containing the corresponding labels, so we shall build these separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    }
   ],
   "source": [
    "%xmode plain\n",
    "\n",
    "import pandas as pd\n",
    "from random import randint, uniform, gauss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from pandas import DataFrame, Series\n",
    "from matplotlib.pylab import plot as plt\n",
    "from numpy import asarray\n",
    "\n",
    "\n",
    "seats = beanchs = 5000\n",
    " \n",
    "data = [[randint(0, 5), # Possible colors for seats\n",
    "         uniform(2, 5), # Possible leg lengths for seats\n",
    "         gauss(2,0.25)] # Possible top surface areas for seats\n",
    "        for i in range(seats)] \\\n",
    "     + [[randint(0, 5), # Six possible colors for beanchs\n",
    "         uniform(4,10), # Possible leg lengths for beanchs\n",
    "           gauss(5, 1)] # Possible top surface areas for beanchs\n",
    "        for i in range(beanchs)]\n",
    " \n",
    "labels = asarray(['seat']*seats + ['beanch']*beanchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 10,000 labelled records of our furnitures, we can initialize a random forest classifier and train it on this data. I have arbitrarily chosen to use 100 estimators (trees in our forest). Unless you’re seeking the most concise representation, as can often be the goal in the sciences, the more estimators the merrier as that tends to lead to better performance — but training and testing time can be a limiting factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform cross-validation to see how well this classifier performs when distinguishing seats from beanchs. This technique involves segregating the data into training and testing samples, fitting the classifier with the training data, and evaluating it on the left out testing data. We can repeat this several times and then average over the results to get a sense of how well this classifier would perform on as yet unseen data. Below I have (again, arbitrarily) chosen to do 100 trials of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(rf_classifier, data, labels, cv=100)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\"\n",
    "      % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a little too good, it gets them all right all the time! Let’s make our data noisy so that we can see how robust it is. Let’s change every 100th beanch to a seat and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99 (+/- 0.14)\n"
     ]
    }
   ],
   "source": [
    "labels[:seats][::100]='beanch'\n",
    "labels[seats:seats+beanchs][::100]='seat'\n",
    "\n",
    "rf_classifier.fit(data, labels)\n",
    "scores = cross_val_score(rf_classifier, data, labels, cv=100)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\"\n",
    "      % (scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. The classifier shows its robust to the mislabeled data we introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closing Notes: \n",
    "\n",
    "Machine learning tools like random forest, SVM, neural networks etc. are all used for high performance. They do give high performance, but users generally don’t understand how they actually work. Not knowing the statistical details of the model is not a concern however not knowing how the model can be tuned well to clone the training data restricts the user to use the algorithm to its full potential.  \n",
    "\n",
    "Random forest are more accurate with predictions when compared to a CART/CHAID or some other regression models in many case. These cases generally have high number of predictive variables and huge sample size. This is because it captures the variance of several input variables at the same time and enables high number of observations to participate in the prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
