{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Learning to process and understand text is one of the first steps on the journey to\n",
    "getting meaningful insights from textual data. Though it is important to understand\n",
    "how language is structured and specific text syntax patterns, that alone is not sufficient\n",
    "to be of much use to businesses and organizations who want to derive useful patterns\n",
    "and insights and get maximum use out of their vast volumes of text data.  \n",
    "\n",
    "One of the most relevant and challenging problems is text classification or\n",
    "categorization, which involves trying to organize text documents into various categories\n",
    "based on inherent properties or attributes of each text document. This is used in\n",
    "various domains, including email spam identification and news categorization. The\n",
    "concept may seem simple, and if you have a small number of documents, you can look\n",
    "at each document and gain some idea about what it is trying to indicate. Based on\n",
    "this knowledge, you can group similar documents into categories or classes. It’s more\n",
    "challenging when the number of text documents to be classified increases to several\n",
    "hundred thousands or millions. This is where techniques like feature extraction and\n",
    "supervised or unsupervised ML come in handy. Document classification is a generic\n",
    "problem not limited to text alone but also can be extended for other items like music,\n",
    "images, video, and other media.  \n",
    "\n",
    "To formalize our problem more clearly, we will have a given set of classes or\n",
    "categories and several text documents. Remember that documents are basically sentences\n",
    "or paragraphs of text. This forms a corpus. Our task would be to determine which class\n",
    "or classes each document belongs to.  \n",
    "\n",
    "![](http://www.kdnuggets.com/wp-content/uploads/text-analysis-acme2.jpg)\n",
    "\n",
    "## What Is Text Classification?\n",
    "\n",
    "Before we define text classification, we need to understand the scope of textual data and\n",
    "what we really mean by classification. The textual data involved here can be anything\n",
    "ranging from a phrase, sentence, or a complete document with paragraphs of text, which\n",
    "can be obtained from corpora, blogs, or anywhere from the Web. Text classification is\n",
    "also often called document classification just to cover all forms of textual content under\n",
    "the word document. The word document could be defined as some form of concrete\n",
    "representation of thoughts or events that could be in the form of writing, recorded\n",
    "speech, drawings, or presentations. I use the term document here to represent textual\n",
    "data such as sentences or paragraphs belonging to the English language.  \n",
    "\n",
    "Text or document classification is the process of assigning text documents into one\n",
    "or more classes or categories, assuming that we have a predefined set of classes.\n",
    "Documents here are textual documents, and each document can contain a sentence or\n",
    "even a paragraph of words. A text classification system would successfully be able to\n",
    "classify each document to its correct class(es) based on inherent properties of the\n",
    "document.  \n",
    "\n",
    "There are a few types of text classification based on the number of classes to predict\n",
    "and the nature of predictions. These types of classification are based on the dataset, the\n",
    "number of classes/categories pertaining to that dataset, and the number of classes that\n",
    "can be predicted on any data point:  \n",
    "\n",
    "- **Binary classification** is when the total number of distinct classes\n",
    "or categories is two in number and any prediction can contain\n",
    "either one of those classes.  \n",
    "- **Multi-class classification**, also known as multinomial\n",
    "classification, refers to a problem where the total number of\n",
    "classes is more than two, and each prediction gives one class\n",
    "or category that can belong to any of those classes. This is an\n",
    "extension of the binary classification problem where the total\n",
    "number of classes is more than two.  \n",
    "- **Multi-label classification** refers to problems where each prediction\n",
    "can yield more than one outcome/predicted class for any data\n",
    "point.  \n",
    "\n",
    "In this notebook I would like to highlight a great example. In the summer of 2016, two interesting NLP papers were published by Facebook Research, [Bojanowski et al., 2016](https://arxiv.org/abs/1607.04606) and [Joulin et al., 2016](https://arxiv.org/abs/1607.01759). The first one proposed a new method for word embedding and the second one a method for text classification. The authors also opensourced a C++ library with the implementation of these methods, [fastText](https://github.com/facebookresearch/fastText), that rapidly attracted a lot of interest.  \n",
    "\n",
    "In this notebook we will discuss how to easily implement several projects using a python wrapper of fastText, [fastText.py](https://github.com/salestock/fastText.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Exception reporting mode: Plain\n",
      "3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:09:58) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%xmode plain\n",
    "\n",
    "import os,sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from urllib.request import urlopen \n",
    "from html import unescape\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "The first task will be to perform text classification dataset DBPedia, which can be accessed [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). The dataset consists of text descriptions of 14 different classes. The training set contains 560,000 reviews and the test contains 70,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                               name  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                         description class_name  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...    Company  \n",
       "1   Schwan-STABILO is a German maker of pens for ...    Company  \n",
       "2   Q-workshop is a Polish company located in Poz...    Company  \n",
       "3   Marvell Software Solutions Israel known as RA...    Company  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...    Company  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set dataset path\n",
    "#dbpedia_csv.tar.gz needs to be downloaded\n",
    "data_path = ''\n",
    "\n",
    "train_file = data_path + 'dbpedia_train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "test_file = data_path + 'dbpedia_test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Mapping from class number to class name\n",
    "class_dict={\n",
    "1:'Company',\n",
    "2:'EducationalInstitution',\n",
    "3:'Artist',\n",
    "4:'Athlete',\n",
    "5:'OfficeHolder',\n",
    "6:'MeanOfTransportation',\n",
    "7:'Building',\n",
    "8:'NaturalPlace',\n",
    "9:'Village',\n",
    "10:'Animal',\n",
    "11:'Plant',\n",
    "12:'Album',\n",
    "13:'Film',\n",
    "14:'WrittenWork'\n",
    "}\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "      <th colspan=\"4\" halign=\"left\">14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>...</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_name</th>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Company</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>EducationalInstitution</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Album</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Film</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>WrittenWork</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>40000</td>\n",
       "      <td>39996</td>\n",
       "      <td>MegaPath Corporation—headquartered in Pleasan...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>39992</td>\n",
       "      <td>Akuressa Training Center of National Youth Se...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>Before Smile Empty Soul became Smile Empty So...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Koryo Celadon is a 1979 American short docume...</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>39984</td>\n",
       "      <td>Tom Clancy's Net Force Explorers or Net Force...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Tsokkos</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Christ's School</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>Indispensable: The Best of Michael Franks</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Netaji Subhas Chandra Bose: The Forgotten Hero</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>A Star Called Henry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "class           1                                                             \\\n",
       "             count unique                                                top   \n",
       "class_name   40000      1                                            Company   \n",
       "description  40000  39996   MegaPath Corporation—headquartered in Pleasan...   \n",
       "name         40000  40000                                            Tsokkos   \n",
       "\n",
       "class                  2          \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  39992   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     3   \\\n",
       "                                                           top   freq  count   \n",
       "class_name                              EducationalInstitution  40000  40000   \n",
       "description   Akuressa Training Center of National Youth Se...      2  40000   \n",
       "name                                           Christ's School      1  40000   \n",
       "\n",
       "class               ...                                                   12  \\\n",
       "            unique  ...                                                  top   \n",
       "class_name       1  ...                                                Album   \n",
       "description  40000  ...     Before Smile Empty Soul became Smile Empty So...   \n",
       "name         40000  ...            Indispensable: The Best of Michael Franks   \n",
       "\n",
       "class                  13         \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  40000   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     14  \\\n",
       "                                                           top   freq  count   \n",
       "class_name                                                Film  40000  40000   \n",
       "description   Koryo Celadon is a 1979 American short docume...      1  40000   \n",
       "name            Netaji Subhas Chandra Bose: The Forgotten Hero      1  40000   \n",
       "\n",
       "class                                                                         \n",
       "            unique                                                top   freq  \n",
       "class_name       1                                        WrittenWork  40000  \n",
       "description  39984   Tom Clancy's Net Force Explorers or Net Force...     15  \n",
       "name         40000                                A Star Called Henry      1  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = df.groupby('class')\n",
    "desc.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to treat the data. We have to create an intermediate file. This intermediate file doesn't have commas, non-ascii characters and everything is lowercase. The changes are based on [this script](https://github.com/facebookresearch/fastText/blob/a88344f6de234bdefd003e9e55512eceedde3ec0/classification-example.sh#L17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataframe, shuffle=False, encode_ascii=False, clean_strings = False, label_prefix='__label__'):\n",
    "    # Transform train file\n",
    "    df = dataframe[['name','description']].apply(lambda x: x.str.replace(',',' '))\n",
    "    df['class'] = label_prefix + dataframe['class'].astype(str) + ' '\n",
    "    if clean_strings:\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\"',''))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\\'',' \\' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('.',' . '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('(',' ( '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(')',' ) '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('!',' ! '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('?',' ? '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(':',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(';',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.lower())\n",
    "    if shuffle:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "    if encode_ascii :\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8'))\n",
    "    df['name'] = ' ' + df['name'] + ' '\n",
    "    df['description'] = ' ' + df['description'] + ' '\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.75 s, sys: 744 ms, total: 10.5 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df, True, False)\n",
    "df_test_clean = clean_dataset(df_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = data_path + 'dbpedia.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = data_path + 'dbpedia.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is cleaned, the next step is to train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 1.3 s, total: 1min 21s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a classifier\n",
    "output_file = data_path + 'dp_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, label_prefix='__label__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can test its accuracy. We can obtain the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model. High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.9797857142857143\n",
      "R@1: 0.9797857142857143\n",
      "Number of examples: 70000\n",
      "CPU times: user 564 ms, sys: 0 ns, total: 564 ms\n",
      "Wall time: 562 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check how the model works with real sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.\n",
      "Label: 3; label name: Artist\n",
      "Sentence:  One of my favourite tennis players in the world is Rafa Nadal.\n",
      "Label: 4; label name: Athlete; certainty: 0.904297\n",
      "Sentence:  Say what one more time, I dare you, I double-dare you motherfucker!\n",
      "Label: 12; label name: Album; certainty: 0.287109\n",
      "Label: 14; label name: WrittenWork; certainty: 0.246094\n",
      "Label: 1; label name: Company; certainty: 0.240234\n"
     ]
    }
   ],
   "source": [
    "sentence1 = ['Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.']\n",
    "labels1 = classifier.predict(sentence1)\n",
    "class1 = int(labels1[0][0])\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %d; label name: %s\" %(class1, class_dict[class1]))\n",
    "\n",
    "sentence2 = ['One of my favourite tennis players in the world is Rafa Nadal.']\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] \n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))\n",
    "\n",
    "#a dialouge from pulp fiction :-)\n",
    "sentence3 = ['Say what one more time, I dare you, I double-dare you motherfucker!']\n",
    "number_responses = 3\n",
    "labels3 = classifier.predict_proba(sentence3, k=number_responses)\n",
    "print(\"Sentence: \", sentence3[0])\n",
    "for l in range(number_responses):\n",
    "    class3, prob3 = labels3[0][l]\n",
    "    print(\"Label: %s; label name: %s; certainty: %f\" %(class3, class_dict[int(class3)], prob3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts the first sentence as `Artist`, which is correct. The second sentence is also predicted correctly. This time we used the function `predict_proba` that returns the certainty of the prediction as a probability. Finally, sentence 3 was not correctly classified. The correct label would be `Film`, since the sentence is from a famous scene of a very good film."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing words: \n",
    "\n",
    "In this notebook we have shown how to classify text.  \n",
    "\n",
    "Text classification is indeed a powerful tool, and we have covered some of the most important aspects related to it in this notebook. We started off our journey with look at the definition and\n",
    "scope of text classification. Next, we defined automated text classification problem and looked at the various types of text classification and finally implemented a text classifier on a real world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
